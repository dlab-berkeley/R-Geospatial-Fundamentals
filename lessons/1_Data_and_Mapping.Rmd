---
title: "Spatial Data and Mapping"
output: html_document
date: "2025-02-18"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
```

# Learning Objectives

Welcome to R Geospatial Fundamentals. Our goals for today's workshop are:

1.  Define **geospatial data** and understand the structure of **spatial dataframes**
2.  Understand **Coordinate Reference Systems (CRS)**
3.  Effectively represent data using **color, symbols, and other visual elements** to highlight patterns in spatial data
4.  Understand how **data transformation** and **classification schemes** can be used to emphasize key aspects of geospatial data

------------------------------------------------------------------------

Throughout this workshop series, we will use the following icons:

ğŸ”” **Question**: A quick question to help you understand what's going on.

ğŸ¥Š **Challenge**: Interactive exercise. We'll go through these in the workshop!

âš ï¸ **Warning**: Heads-up about tricky stuff or common mistakes.

ğŸ’¡ **Tip**: How to do something a bit more efficiently or effectively.

ğŸ“ **Poll**: A zoom poll to help you learn.

ğŸ¬ **Demo**: Showing off something more advanced so you know what you can use R for in the future

------------------------------------------------------------------------

# Part 1: Introducing Geospatial Data

## 1.1 Defining Geospatial Data

Geospatial data is information encoded with a specific location using coordinates. It is made of attributes (what), representing the data associated a particular location (where). These attributes could range from environmental factors to demographic details.

**Example:** Alameda High School

**Attributes (what):** number of students, year founded, school district etc.

**Location (where):** 37Â°45'49"N 122Â°14'49"W

![](../images/Alameda_High_School_Google_Earth.png)

ğŸ”” **Question**: Is the elevation shown in this image part of the attributes (what) or the location (where)?

Geospatial data also often comes with additional data, metadata, that provides context. This could include information about where the geospatial data was obtained (the data source) or the date of last update of the data.

------------------------------------------------------------------------

## 1.2 Spatial Dataframes

This workshop uses the package `sf` ("simple features"). This package works with "spatial dataframes", which are a special type of dataframe that contains spatial information.

```{r}
# Load the sf package (uncomment the `install.packages` line if not already installed)
#install.packages("sf")
library(sf)
```

Let's read in our first spatial file, which is contained in the "data" folder under `california_schools/california_schools.shp`. For this, we will use the `st_read()` function from the `sf` package:

```{r}
schools_sf <- st_read("../data/california_schools.shp")
#data source: https://gis.data.ca.gov/datasets/f7f818b0aa7a415192eaf66f192bc9cc
```

The output we can see when we read in a spatial file is known as "metadata". We will discuss most of what is contained in the metadata bit by bit throughout this workshop.

ğŸ’¡ **Tip**: Spatial data is stored in specific file formats. The [ESRI shapefile](https://en.wikipedia.org/wiki/Shapefile) is the most widely used file format for storing geospatial (vector) data. This has a `.shp` file ending. This shapefile is part of a collection of files, with the same filename and different endings (`.shx`, `.dbf`, and `.prj`). These other files must be stored in the same location as the `.shp` file for the data to be read properly.

Working with spatial dataframes is similar to working with base R dataframes. A spatial (or sf) dataframe retains all of the functionality of an R `data.frame`, extended with a `geometry` column and metadata about this column.

Let's explore some basic aspects of a spatial dataframe:

```{r}
# Look at the type of R objects an sf dataframe is 
class(schools_sf)

# Look at the size of the dataframe
dim(schools_sf) 

# View the dataframe
View(schools_sf) 
```

When referencing geospatial data (like the `schools_sf` dataframe):

-   Columns containing data are referred to as **attributes**.

-   Rows are referred to as **features**.

-   The `geometry` column provides us with spatial information for each feature.

ğŸ”” **Question**: What is an example of an **attribute** in the `schools_sf` dataframe?

------------------------------------------------------------------------

## 1.3 Identifying Geometry Types

There are three main types of geometries. These are distinct spatial characteristics that can be associated with a spatial dataframe. They are points, lines and polygons:

![](https://datacarpentry.org/organization-geospatial/fig/dc-spatial-vector/pnt_line_poly.png)

Let's look at the `geometry` column.

```{r}
# View the geometric information of this spatial dataframe 
st_geometry(schools_sf)
```

This summary includes:

-   Number of **features** (or rows)

-   The **Geometry type** of geometry---POINT---referring to the location of the schools.

-   The **Dimension** describes the data. We have 2 dimensions (2-D): the X and Y axis.

-   The **Bounding Box** refers to the outer limits in the coordinate system (XY) that contain, or bound, the data.

-   **Projected CRS** is covered in the next section

-   The geometries for the first 5 features are highlighted in red

------------------------------------------------------------------------

### ğŸ¥Š Challenge 1: Exploring More Complex Geometry Types

For this challenge, let's read in a new shapefile and store it as the object `counties`.

1.  Read in the file `../data/california_counties.shp` (hint: follow the same filepath format as for the schools_sf object)
2.  Look at the data: what information does this dataframe contain?
3.  Identify the geometry type.

```{r}

# Step 1: Read in california_counties.shp and store as `counties`
counties <- ______

# Step 2: Look at the first few rows of the data


# Step 3: Look at the geometry column and identify the geometry type


```

ğŸ’¡ **Tip**: Geometry types (`point`, `line`, `polygon`) have their multi-equivalents (`multiline`, `multipoint`, & `multipolygon`). These account for irregular or complex shapes, disconnected or intersecting boundaries.

ğŸ”” **Question**: Why would data based on county boundaries be best represented as a multipolygon?

------------------------------------------------------------------------

## 1.4 Navigating Coordinate Reference Systems

A coordinate reference system (CRS) is a system for associating a position on the surface of the Earth, with numerical coordinates.

![](../images/prime-meridian-equator-world-map.webp)

There are many different CRS's and types of CRS's because the Earth is a complex surface with curvature and elevation. In representing these 3-dimensional aspects via a 2-dimensional map, some aspects become distorted. All map projections introduce some amount of distortion in **area**, **shape**, **distance** or **direction**.

Let's explore two distinct types CRS's:

------------------------------------------------------------------------

### 1.4.1 Geographic vs. projected CRS

![](../images/GCS_PCS.png)

**Geographic Coordinate Systems...**

-   are designed to maintain the shape of continents.

-   use latitude and longitude to specify locations on the Earth's surface.

-   are suitable for global references and large-scale datasets.

-   use angular units; degrees.

**Projected Coordinate Systems...**

-   are designed for accurate representation of distances, areas, angles and shapes.

-   use x and y coordinates on a flat surface for mapping.

-   are suitable for mapping and navigation.

-   use linear units; feet, meters etc.

Various projection methods are used to preserve specific properties, such as area, shape, distance, or direction, depending on the application requirements. Let's explore some common CRS's.

### 1.4.2 CRS Codes

CRS's are referenced by a common names and, in software, by numeric codes, often called EPSG codes.

-   Common **geographic** CRS codes include "WGS84" (EPSG 4326) and "NAD83" (EPSG 4269)--the default for US census data.

Let's see some instances where different CRS's are used:

```{r}
# View the CRS of the spatial dataframe
st_crs(counties)
```

The first line of output tells us that the name of this CRS is NAD83. The final line tells us that the EPSG code is 4269.

```{r}
# View the CRS of the spatial dataframe
st_crs(schools_sf)
```

The first line of output tells us that the name of this CRS is WGS 84 / Pseudo-Mercator. The final line tells us that the EPSG code is 3857.

### 1.4.3 CRS Reprojections

Let's reproject the CRS of our data such that they match and can be analyzed together. We will use the `st_transform` function to do this, and check if the CRS's are the same.

```{r}
# First, get CRS of the spatial dataframe 
st_crs(counties) # NAD83
st_crs(schools_sf) # WGS84 / Pseudo-Mercator

# Check if the CRS's are the same
st_crs(counties) == st_crs(schools_sf)
```

We can see that the CRS's of our two objects don't match. We can use the `st_transform` function to change the CRS of one of our objects.

```{r}
# Change the CRS of the schools spatial dataframe to a different CRS based on a known EPSG code
schools_sf_4269 = st_transform(schools_sf, crs = 4269) 

st_crs(schools_sf_4269)
```

ğŸ’¡ **Tip**: If you don't know the EPSG code, you can set the CRS based on an existing spatial dataframe. In this case, we're setting the CRS based on the counties dataframe.

```{r}
# Change the CRS of the schools spatial dataframe to match the counties dataframe
schools_sf_4269 = st_transform(schools_sf, crs=st_crs(counties))
```

------------------------------------------------------------------------

## 1.5 Creating Spatial Dataframes

So far, we've dealt with 'preformatted' data, that comes as a spatial dataframe already. But we can also *create* a spatial dataframe!

```{r}
# Read in a CSV file containing information on schools in Alameda county
alameda_schools_df <- read.csv("../data/alameda_schools.csv")

# Examine the contents of the dataframe
head(alameda_schools_df)
```

This dataframe does *not* include a `geometry` column because it was read from a non-spatial CSV file. This means that it is not currently in the spatial `sf` format:

```{r}
class(alameda_schools_df)
```

Non-spatial data from a spreadsheet (such as a CSV) can be transformed in to a spatial dataframe with just 2 pieces of information:

-   Columns that specify the geometry (e.g. point coordinates) associated with each feature: in this case, the columns `X` and `Y`

-   A CRS for the data: we can choose this for ourselves.

With this information, we can use the function `st_as_sf` to transform a spreadsheet data into an `sf` spatial dataframe.

```{r}

# Convert the .csv into a spatial dataframe and set the CRS of the dataframe
alameda_schools_sf <- st_as_sf(alameda_schools_df, 
                               coords = c('X','Y'), # Column names containing location data
                               crs = 4326) # CRS of the data

# Check the class of the dataframe
class(alameda_schools_sf)
```

The resulting dataframe `alameda_schools_sf` is now a spatial dataframe ("sf")! If we look at the contents of the dataframe, we can see that the columns `X` and `Y` have been replaced by a `geometry` column.

```{r}
head(alameda_schools_sf)
```

## 1.6 Saving Spatial Dataframes

It's helpful to be able to save data, especially if it has been made into a spatial dataframe or transformed as we did here, in a format that streamlines continued manipulation and analysis.

The `st_write()` function is used to save spatial dataframes in the available file types (e.g. `.shp`)

```{r}

# Save to shapefile
st_write(alameda_schools_sf,                            # Object
         "../data/alameda_schools.shp", # Filepath
         #delete_dsn = TRUE                             # Replace existing version of this file (if applicable)
         )                             
```

Note that this command creates an .shp file, and that several auxiliary files (.dbf, .prj, .shx) are also created. These auxiliary files must be retained in the same folder as the .shp file.

Other geospatial file types include [.geojson](https://geojson.org/) or .[gpkg](https://www.geopackage.org/). We can save in one of these other file formats simply by changing the file ending.

```{r}

# Save to geopackage
st_write(alameda_schools_sf,                             # Object
         "../data/alameda_schools.gpkg", # Filepath
         #delete_dsn = TRUE                              # Replace existing version of this file (if applicable)
         )                             
```

------------------------------------------------------------------------

# Part 2: Geospatial Data Visualization

One of the most powerful characteristics of geospatial data is our ability to create spatial visualizations of attributes. This can reveal spatial patterns such as **clusters**, and spatial relationships such as **proximity**.

## 2.1 Plotting Geometries

We'll start off using the basic `plot` function.

```{r}

# Plot the schools_sf dataframe
plot(schools_sf) 
```

âš ï¸ **Warning**: The default `plot` function attempts to plot all the data in the dataset at once! It's recommended to plot the `$geometry` column of spatial data, and not the entire dataset. The latter can take a lot of time to execute particularly for larger datasets.

Let's examine just the geometry of our `schools_sf` dataframe:

```{r}

# Plot the geometry column of the schools_sf dataframe
plot(schools_sf$geometry) 
```

ğŸ”” **Question**: Just based on this plot, can you tell what type of geometry the `schools_sf` dataframe represents (point, line, or polygon)?

Now let's do the same for our `counties`:

```{r}

# Plot the geometry column of the counties dataframe
plot(counties$geometry) 
```

ğŸ”” **Question**: What type of geometry does our `counties` dataframe represent (point, line, or polygon)?

We can also overlay these two geometries in the same plot by using the `add = TRUE` argument.

```{r}
# Run these two lines together
plot(counties$geometry)
plot(schools_sf$geometry, add=TRUE)
```

This didn't work! What happened?

Before we create the overlay plot, we should always check that the CRS of our dataframes match!

```{r}
# Check if the two datasets are in the same CRS
st_crs(schools_sf) == st_crs(counties) 
```

They don't match! This is why the schools_sf layer didn't display on our plot. If two spatial dataframes have different CRS, they are often being mapped on totally different scales and cannot be plotted together. We can fix this by putting both layers in the same CRS.

Recall we already created a version of the schools_sf dataframe that has the same CRS as the counties dataframe!

```{r}
# Check if the two datasets are in the same CRS
st_crs(schools_sf_4269) == st_crs(counties) 
```

âš ï¸ **Warning**: Plotting data with mismatched CRS can be a simple, common error. Build a habit of always checking the CRS of a dataset before doing any analysis, including plotting.

Now, let's create the overlay plot:

```{r}
# Run these two lines together
plot(counties$geometry)
plot(schools_sf_4269$geometry, add=TRUE)
```

------------------------------------------------------------------------

## 2.2 Thematic Maps

So far, we have simply plotted the location of features/observations in space.

The goal of a thematic map is to layer information about the spatial distribution of a variable onto a map, allowing us to identify trends, outliers, etc.

Thematic maps use **color** to quickly and effectively convey information. For example, maps use brighter or richer colors to signify higher values, and leverage cognitive associations such as mapping water with the color blue. These maps visually communicate spatial patterns, enabling intuitive interpretation and comparison of patterns and data distributions.

Let's compare two visualizations of median age by county: 1) a standard bar plot, and 2) a thematic map. For this, we'll load the `ggplot()` package for more sophisticated data visualization.

```{r}

# Load the ggplot2 package (uncomment the `install.packages` line if not already installed)

#install.packages("ggplot2")
library(ggplot2)
```

Visualize the median age per county as a barplot using ggplot:

```{r}

# Plot of MED_AGE by county

ggplot(data = counties) +                                   # Initialize ggplot using counties dataframe
  geom_col(aes(x = NAME, y = MED_AGE)) +                    # Plot the data as bar plot 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust the angle of the x-axis text for easier visualization
```

ğŸ”” **Question**: Alameda and San Francisco county neighbor one another. How easy is it to relate the median ages of the two counties using this bar plot?

Visualizing the data using this plot emphasizes numerical relationships, while a thematic map places additional emphasis on the geographic distribution of data. Herein lies one of the key benefits of geospatial analyses.

Let's use `ggplot2`'s `geom_sf()` function to visualize the median age per county on a map.

```{r}
ggplot(counties) +              # Initialize ggplot using counties dataframe
  geom_sf(aes(fill = MED_AGE))  # Use median age for fill color
```

ğŸ”” **Question**: What patterns do you observe in this plot? Are low and high median ages clustered in certain parts of the state?

## 2.3 Color Palettes

There are three main types of color palettes, each of which has a different purpose: diverging, sequential, and qualitative.

![](http://www.gnuplotting.org/figs/colorbrewer.png)

ğŸ’¡ **Tip**: Sites like [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=Blues&n=3) let you play around with different types of color maps.

Let us visualize the same data using all 3 types of palettes.

```{r}
library(RColorBrewer)
```

To see the names of all color palettes available, try the following command. You may need to enlarge the output image to see the palette names.

```{r}
RColorBrewer::display.brewer.all()
```

Let's visualize each of these three color palettes with our counties dataset.

1.  **Sequential color palettes** use a single or multi-color hue to emphasize differences in order and magnitude.

```{r}

seq_plot <- ggplot(data = counties) +                     # Initialize ggplot
  geom_sf(aes(fill = MED_AGE)) +                          # Use median age for fill color
  scale_fill_gradientn(colors = brewer.pal(9,"Greens")) + # *Sequential* greens color palette
  theme_minimal() +                                       # Add theme
  labs(title = "Median Age per County")                   # Add title
  
seq_plot
```

This is considered a `proportional color map` because the colors are linearly scaled to the data values. The legend has a continuous color ramp rather than discrete data ranges.

2.  **Diverging color palettes** use a "diverging" set of colors to emphasize the distribution of values relative to some midpoint value.

[Plot data with a diverging palette]{.underline}

```{r}

div_plot <- ggplot(data = counties) +                     # Initialize ggplot
  geom_sf(aes(fill = MED_AGE)) +                          # Use median age for fill color
  scale_fill_gradientn(colors = brewer.pal(11, "RdBu")) + # *Diverging* red-blue color palette
  theme_minimal() +                                       # Add theme
  labs(title = "Median Age per County")                   # Add title

div_plot
```

ğŸ”” **Question**: What are possible advantages of the diverging color scheme over the sequential color scheme? What are possible disadvantages?

3.  **Qualitative color palettes** use a contrasting set of colors to identify distinct categories and avoid quantitative significance

```{r}

qual_plot <- ggplot() +
  geom_sf(data = counties, aes(fill = MED_AGE)) + 
  scale_fill_manual(values = brewer.pal(8, "Pastel2")) + # *Qualitative* pastel color palette
  theme_minimal() +
  labs(title = "Selected Counties")

qual_plot
```

ğŸ”” **Question**: Why is this not working?

Visualizing a continuous variable with a qualitative plot [does not work]{.underline}. We need to use a qualitative category instead. Here's what it would look like with a qualitative category - in this case, Combined Statistical Area:

```{r}

#plot the data
qual_plot <- ggplot() +
  #since we are using a qualitative color palette, we cannot plot quantitative data
  geom_sf(data = counties, aes(fill = METRO)) + 
  scale_fill_manual(values = brewer.pal(8, "Pastel2")) + #Qualitative pastel color pallette
  theme_minimal() +
  labs(title = "Selected Counties")
qual_plot
```

âš ï¸ **Warning**: It is always important to choose the correct color palette for your thematic map. In particular, a qualitative color palette should almost never be used for continuous quantitative data.

------------------------------------------------------------------------

### ğŸ“ Poll: Choosing the Appropriate Color Palette

Which color palette would most effectively represent the following variables?

1.  The percent of students that are socioeconomically disadvantaged
2.  Whether or not a given school is a charter school (Y/N)
3.  Change in total enrollment between 2019 and 2024

------------------------------------------------------------------------

## 2.4 Data Transformation

There are two major challenges when creating thematic maps:

1.  Our eyes are drawn to the color of larger areas or linear features, even if the values of smaller features are more significant.

2.  The range of data values is rarely evenly distributed across all observations, so colors can be misleading.

Selecting the appropriate color palette can help mitigate these challenges. Sometimes, this alone is not enough. Transforming data can improve the way data values are associated with colors. Let's explore three different methods for visualizing the spatial distribution of people that identify as multi-racial: **counts**, **densities**, and **proportions**.

**A. Visualizing Data as Counts**

Count data are individual-level data (e.g. population), aggregated by feature (e.g. county). These plots emphasize the areas with the largest absolute counts.

```{r}

count_plot <- ggplot(data = counties) +                     # Initialize ggplot
  geom_sf(aes(fill = MULTI_RACE)) +                         # Fill color based on MULTI_RACE
  scale_fill_gradientn(colors = brewer.pal(9, "Greens")) +  # Set color gradient
  theme_minimal() +                                         # Set theme
  labs(title = "Multi-Racial Population (Count)")           # Add title

count_plot
```

ğŸ”” **Question**: If our goal is to identify the distribution of multi-racial individuals, what might be the problem with mapping total counts?

**B. Visualizing data as Densities**

Density data is counts aggregated by feature (county) and [normalized]{.underline} (divided) by the geographia area of the feature.

For example, we could look at the population per square mile within a county:

```{r}

# Create a new column that contains the multi_race density data
counties$MULTI_RACE_DENSITY <- counties$MULTI_RACE/counties$AREA_SQMI

density_plot <- ggplot(data = counties) +                               # Initialize ggplot
  geom_sf(aes(fill = MULTI_RACE_DENSITY)) +                             # Fill color based on MULTI_RACE
  scale_fill_gradientn(colors = brewer.pal(9, "Greens")) +              # Set color gradient
  theme_minimal() +                                                     # Set theme
  labs(title = "Multi-Racial Population (Density)")                     # Add title

density_plot
```

ğŸ”” **Question**: If our goal is to identify the distribution of multi-racial individuals, what might be the problem with mapping densities?

**C. Visualizing Data as Percents/Proportions**

Normalizing data via densities makes the data more accurate in some cases and more comparable across regions of different sizes. Normalizing data via percentages allows for a direct comparison of the relative contribution, irrespective of size or population.

**Percent/Proportion data** represents data in a specific category divided by the total value across all categories: for example, the number of multi-racial individuals divided by the total population.

```{r}

#create a new column that contains the multi_race proportion data
counties$MULTI_RACE_PERCENT <- 100*counties$MULTI_RACE/counties$POP

percent_plot <- ggplot(data = counties) +                               # Initialize ggplot
  geom_sf(aes(fill = MULTI_RACE_PERCENT)) +                             # Fill color based on MULTI_RACE
  scale_fill_gradientn(colors = brewer.pal(9, "Greens")) +              # Set color gradient
  theme_minimal() +                                                     # Set theme
  labs(title = "Multi-Racial Population (Percent)")                     # Add title

percent_plot
```

### ğŸ¥Š Challenge 2: **Visualizing Race by County**

Now it's your turn! Using the method shown above, plot the distribution of another race group of your choice from the `counties` dataset using all three methods: 1) count, 2) density, and 3) percent.

```{r}


```

Describe any patterns you notice in the chat!

------------------------------------------------------------------------

## 2.5 Classification Schemes

Another way to make more meaningful maps is to improve the way in which data values are associated with colors.

The common alternative to the proportional color maps we've created thus far is to use a **classification scheme** to create a **graduated color map**.

A **classification scheme** is a method for binning continuous data values into multiple categories and then associating each category with a different color in a color palette.

Commonly used classification schemes include equal interval, quantiles, and natural breaks. Let's explore each of these three classifications schemes for population density by county!

For ease of use, we're going to use a different mapping package known as `tmap`, which makes exploring different classifications a bit more straightforward. The format of this package is similar to `ggplot2`, but differs in its specific syntax.

```{r}
# install.packages("classInt")
library(tmap)
```

**A. Equal Intervals**

Equal-interval classification separates data into equal-size data ranges (e.g., values within 0-10, 10-20, 20-30, etc.)

-   Advantages:
    -   Works well for data spread across the entire range of values
    -   Easily understood by map readers
-   Disadvantages:
    -   Heavily influenced by skewed data and outliers, which may result in one or more of the bins having no observations

Let's look at a map of county population based on equal intervals. Note the differences between `tmap` and `ggplot2` syntax:

```{r}
equal_plot <- tm_shape(counties) +                   # Initialize tmap object
  tm_polygons(col = 'POP',                           # Name of column for fill color
              style = "equal",                       # Classification scheme
              palette = "YlOrRd",                    # Color palette
              title = "Population (Equal Interval)") # Title
equal_plot
```

ğŸ”” **Question**: What are the limitations of this approach for conveying useful information about county population?

**B. Quantiles**

Instead of creating equally-sized bins, a quantile scheme distributes an equal *number of observations* into each bin. For example, if we had 15 observations and 3 bins, each bin would get 5 observations (low, medium, and high)

-   Advantages:
    -   Ensures that all map colors are used, resulting in more visually effective displays of skewed data
    -   Provides a relative indication of where observations fall within the overall distribution
-   Disadvantages:
    -   Because quantiles are based solely on number of observations, different categories may have narrow or wide ranges of values
    -   Quantile categories necessarily obscure outliers, which may be valuable to distinguish

```{r}
quantile_plot <- tm_shape(counties) +                   # Initialize tmap object
  tm_polygons(col = 'POP',                              # Name of column for fill color
              style = "quantile",                       # Classification scheme
              palette = "YlOrRd",                       # Color palette
              title = "Population (Quantile)")    # Title
quantile_plot
```

ğŸ”” **Question**: What are the limitations of this approach for conveying useful information about county population? (Hint: look at the relative ranges of the lowest and highest categories)

**C. Natural Breaks**

Natural Breaks classification methods minimize within-class variance and maximize between-class differences. Don't worry too much about the mathematical nuances here; the bottom line is that this method identifies the biggest "gaps" in the distribution so values that are more similar are grouped together within the same bin. We will look at the "fisher" natural breaks method - other common natural breaks methods include "jenks".

```{r}
fisher_plot <- tm_shape(counties) +                   # Initialize tmap object
  tm_polygons(col = 'POP',                            # Name of column for fill color
              style = "fisher",                       # Classification scheme
              palette = "YlOrRd",                     # Color palette
              title = "Population (Natural Breaks)")  # Title
fisher_plot
```

This map shares some advantages *and* disadvantages of the equal interval and quantile methods.

-   Advantages:
    -   Ensures that all map colors are used, resulting in more visually effective displays of skewed data
    -   Effectively highlights observations
-   Disadvantages:
    -   Different categories may have narrow or wide ranges of values
    -   Categories may be more difficult for map readers to interpret

**D. Classifying data Manually**

If we aren't happy with any of these methods, one final option is to create our own manual categories. For example, we might want to better differentiate between counties with populations under 1 million:

```{r}

manual_plot <- tm_shape(counties) +                  # Initialize tmap object
  tm_polygons(col = "POP",                           # Name of column for fill color
              style = "fixed",                       # Classification scheme
              palette = "YlOrRd",                    # Color palette
              # Manual breaks (including min of 0 and max based on maximum in dataset)
              breaks = c(0, 250000, 500000, 750000, 1000000, max(counties$POP)),
              title = "Population (Manual Breaks)")  # Title
manual_plot
```

------------------------------------------------------------------------

### ğŸ¥Š Challenge 3: Classifying Race by County

For your final challenge, map the race percentage field you created in Challenge 2 using each of these classification methods. Which of these methods do you think is the most effective for displaying your variable?

```{r}

equal_plot2 <- _____

quantile_plot2 <- _____

fisher_plot2 <- _____

manual_plot2 <- _____

```

------------------------------------------------------------------------

# Key Points

-   Spatial dataframe are similar to ordinary data frames, except that each feature (row) has an associated `geometry` column encoding a spatial dimension
-   Spatial geometries consist of points, lines, and polygons, which represent different types of spatial observations
-   Coordinate Reference Systems (CRS) provide various ways to project the complex surface of the earth into a two-dimensional map. Knowing the CRS of your data and which CRS is most applicable in your given context enables accurate data manipulation and integration.
-   Visualizing spatial dataframes can reveal spatial patterns such as clusters, and spatial relationships such as proximity.
-   Color palettes, data transformation, and classification schemes can all be used to enhance the communication of spatial information in thematic maps.
